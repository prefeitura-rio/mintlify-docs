---
title: "Construindo uma Pipeline com Prefect 3"
description: "Guia completo para criar pipelines de dados usando Prefect 3 na Prefeitura do Rio de Janeiro"
---

# Construindo uma Pipeline com Prefect 3

Este guia explica como criar e configurar pipelines de dados usando Prefect 3 na Prefeitura do Rio de Janeiro, seguindo as melhores pr√°ticas e padr√µes estabelecidos pela equipe IplanRio.

## ‚úÖ Pr√©-requisitos

- Acesso de leitura ao projeto BigQuery `rj-iplanrio`
- Permiss√µes de colaborador no reposit√≥rio GitHub
- Conhecimento b√°sico de Git e GitHub
- Python 3.8+ instalado
- `uv` package manager instalado

## üîß Configura√ß√£o Inicial

### 1. Clonar o Reposit√≥rio

```bash
git clone https://github.com/prefeitura-rio/prefect_rj_iplanrio.git
cd prefect_rj_iplanrio
```

## üöÄ Boas Pr√°ticas de Desenvolvimento

### 1. Estrutura de Branch

```bash
# Criar branch para sua pipeline
git checkout -b staging/nome-da-pipeline

# Exemplo para pipeline de sa√∫de
git checkout -b staging/pipeline-dados-saude-sms
```

<Note>
  **IMPORTANTE**: Use sempre o prefixo `staging/` no nome da branch para que o CI/CD reconhe√ßa e processe sua pipeline automaticamente.
</Note>

### 2. Nomenclatura de Pipelines

Siga o padr√£o estabelecido:

```python
# ‚úÖ Correto: Nome descritivo e em snake_case
@flow(log_prints=True)
def rj_cvl__os_info():
    pass

# ‚ùå Evite: Nomes gen√©ricos
@flow(log_prints=True)
def extracao_de_dados():
    pass
```

## üß™ Cria√ß√£o de Nova Pipeline

### 1. Usar Template Cookiecutter

O reposit√≥rio utiliza cookiecutter para facilitar a cria√ß√£o de novas pipelines de forma padronizada. Os templates dispon√≠veis em `templates/` permitem gerar rapidamente a estrutura de diret√≥rios e arquivos necess√°rios para uma nova pipeline Prefect, incluindo Dockerfile, flow.py, prefect.yaml e pyproject.toml.

Para criar uma nova pipeline, instale `uv` e rode:

```bash
uvx cookiecutter templates --output-dir=pipelines
```

Voc√™ ser√° solicitado a informar valores como secretaria e pipeline, que ser√£o utilizados para preencher os nomes dos diret√≥rios, arquivos e vari√°veis nos templates. O template gerado j√° segue o padr√£o de nomenclatura definido anteriormente.

**Exemplo de intera√ß√£o:**

```bash
$ uvx cookiecutter templates --output-dir=pipelines
secretaria [sms]: sms
pipeline [dados_pacientes]: dados_saude_sms
```

### 2. Configurar flow.py

Ap√≥s gerar o template, substitua os valores padr√£o no arquivo `flow.py` pelos valores espec√≠ficos da sua pipeline:

```python
# Exemplo de configura√ß√£o em flow.py
@flow(log_prints=True)
def pipeline_dados_saude_sms():
    # Configura√ß√µes da base de dados
    db_database = "banco1"  # Nome do Banco de Origem
    db_host = "10.15.255.1"  # Host do Banco de Origem
    db_port = 3306  # Porta do Banco de Origem
    db_type = "mysql"  # Tipo do Banco de Origem
    
    # Configura√ß√µes do dataset
    dataset_id = "nome_do_dataset"  # Dataset de destino em rj-iplanrio
    
    # Configura√ß√µes do Infisical
    infisical_secret_path = "/db-seu-banco-de-origem"  # Caminho dos secrets
    
```

**Par√¢metros importantes:**

- `db_database`: Nome do banco de dados de origem (ex: `banco1`)
- `db_host`: Host do banco de dados de origem (ex: `10.15.255.1`)
- `db_port`: Porta de conex√£o do banco de dados de origem (ex: `3306`)
- `db_type`: Tipo do banco de dados de origem (ex: `mysql`, `postgres`, etc)
- `dataset_id`: Dataset de destino no BigQuery (todas as extra√ß√µes s√£o adicionadas ao projeto `rj-iplanrio`)
- `infisical_secret_path`: Caminho dos secrets no Infisical

### 3. Configurar prefect.yaml

Configure os schedules no arquivo `prefect.yaml` conforme suas necessidades:

#### Schedule Overwrite

```yaml
schedules:
  # Exemplo de schedule overwrite
  - interval: 86400 # executa a cada 24h
    anchor_date: "2022-01-01T01:00:00" # Data de inicio do schedule em UTC
    timezone: America/Sao_Paulo
    slug: dados_saude_sms_diario # slug do schedule
    parameters:
      table_id: pacientes_sms # Nome da tabela no BigQuery
      execute_query: |
        SELECT
          id_paciente,
          nome,
          data_nascimento,
          telefone
        FROM banco1.saude
```

#### Schedule Incremental (Incremental)

```yaml
schedules:
  # Exemplo de schedule incremental
  - interval: 86400
    anchor_date: "2022-01-01T02:32:00"
    timezone: America/Sao_Paulo
    slug: dados_saude_sms_incremental
    parameters:
      table_id: pacientes_sms_incremental
      dump_mode: append
      partition_columns: data_atualizacao # Coluna de particionamento
      partition_date_format: "%Y-%m-%d" # Formato da data de particionamento
      break_query_frequency: day # Frequencia de particionamento
      break_query_start: current_day # Data de inicio do particionamento
      break_query_end: current_day # Data de fim do particionamento
      execute_query: |
        SELECT
          id_paciente,
          nome,
          data_nascimento,
          telefone,
          data_atualizacao
        FROM banco1.saude
```

**Explica√ß√£o dos par√¢metros:**

- `interval`: Intervalo entre as runs em segundos (86400 = 24 horas)
- `anchor_date`: Data de in√≠cio do schedule em UTC
- `timezone`: Fuso hor√°rio (America/Sao_Paulo)
- `slug`: Identificador √∫nico do schedule
- `table_id`: Nome da tabela no BigQuery
- `dump_mode`: Modo de inser√ß√£o (`append` para incremental, `overwrite` para substitui√ß√£o)
- `partition_columns`: Coluna(s) para particionamento
- `partition_date_format`: Formato da data de particionamento
- `break_query_frequency`: Frequ√™ncia de particionamento (`day`, `month`, `year`)
- `break_query_start/end`: Per√≠odo de dados a serem processados

### 4. Commit e Push

```bash
# 1. Adicionar arquivos
git add pipelines/sua_secretaria/sua_pipeline/

# 2. Commit com mensagem descritiva
git commit -m "feat: adiciona pipeline de dados de sa√∫de SMS

- Cria pipeline para extra√ß√£o de dados SMS
- Configura schedule di√°rio incremental
- Adiciona particionamento por data"

# 3. Push para branch
git push origin staging/sua-pipeline
```

### 5. Pull Request

1. **Criar PR** no GitHub
2. **Descri√ß√£o**: Explicar mudan√ßas e impacto
3. **Review**: Solicitar revis√£o da equipe IplanRio
4. **Testes**: Garantir que todos os testes passem
5. **Teste em Staging**: Ap√≥s o deploy em staging funcionar, voc√™ j√° pode testar sua pipeline no ambiente de desenvolvimento
6. **Merge**: Ap√≥s aprova√ß√£o, merge para `main`

#### Workflow de CI/CD Autom√°tico

O reposit√≥rio utiliza GitHub Actions para automatizar todo o ciclo de vida das pipelines, incluindo build, deploy e publica√ß√£o de imagens Docker:

**üöÄ Deploy Autom√°tico dos Flows:**

O sistema possui dois workflows principais para deploy autom√°tico:

- **`.github/workflows/deploy-prefect-flows-staging.yaml`** - Ambiente de Staging
  - Acionado a cada push em branches `staging/*`
  - Pode ser executado manualmente
  - Monitora altera√ß√µes em `pipelines/**`

- **`.github/workflows/deploy-prefect-flows-prod.yaml`** - Ambiente de Produ√ß√£o
  - Acionado a cada push na branch `master`
  - Pode ser executado manualmente
  - Monitora altera√ß√µes em `pipelines/**`

**üîß Processo de Deploy:**

Ambos os workflows executam as seguintes etapas:

1. **Checkout do c√≥digo-fonte**
2. **Login no GitHub Container Registry** (ghcr.io)
3. **Instala√ß√£o das depend√™ncias Python** com `uv`
4. **Execu√ß√£o do script de deploy** `.github/scripts/deploy_prefect_flows.py`
   - Faz o deploy autom√°tico de todos os flows definidos em `pipelines/*/prefect.yaml`
   - Caso algum deploy falhe, o workflow √© interrompido e o erro √© registrado nos logs

**üê≥ Build e Publica√ß√£o da Imagem Base:**

O workflow `.github/workflows/build-and-push-root-dockerfile.yaml`:

- **Acionado**: Em altera√ß√µes no Dockerfile da raiz ou push na branch master
- **Execu√ß√£o manual**: Dispon√≠vel quando necess√°rio
- **Processo**:
  - Build da imagem Docker definida no Dockerfile do reposit√≥rio
  - Publica√ß√£o da imagem no GitHub Container Registry (`ghcr.io/${{ github.repository }}:latest`)

<Note>
  O workflow de staging permite testar suas pipelines em um ambiente isolado antes do deploy em produ√ß√£o. 
  Ap√≥s o deploy em staging funcionar, voc√™ j√° pode testar sua pipeline no ambiente de desenvolvimento.
  Apenas ap√≥s o merge na branch master √© que as pipelines s√£o deployadas em produ√ß√£o.
</Note>

<Warning>
  **IMPORTANTE**: Se algum deploy falhar durante o processo, o workflow ser√° interrompido e voc√™ precisar√° corrigir os problemas antes de tentar novamente.
</Warning>

**üìä Monitoramento do CI/CD:**

- Acompanhe o progresso dos deploys na aba "Actions" do seu reposit√≥rio
- Verifique os logs para identificar poss√≠veis erros
- Aguarde a conclus√£o bem-sucedida antes de solicitar review da equipe
- Se houver falhas, corrija os problemas e fa√ßa novo commit - o CI ser√° executado novamente

## üîß Troubleshooting

### Problemas Comuns

**Erro de conex√£o com banco de dados:**
- Verifique se as credenciais est√£o corretas no Infisical
- Confirme se o host e porta est√£o acess√≠veis
- Teste a conex√£o manualmente

**Falha no deploy:**
- Verifique os logs na aba "Actions" do GitHub
- Confirme se todos os arquivos necess√°rios foram commitados
- Valide a sintaxe do `prefect.yaml`

**Pipeline n√£o executa no schedule:**
- Verifique se o `anchor_date` est√° no passado
- Confirme se o `timezone` est√° correto
- Valide se o `interval` est√° em segundos

**Dados n√£o aparecem no BigQuery:**
- Verifique se o `dataset_id` est√° correto
- Confirme se a query est√° retornando dados
- Valide as permiss√µes de escrita no projeto `rj-iplanrio`

## üìö Recursos Adicionais

- [Migra√ß√£o de Pipeline](https://iplan-rio.mintlify.app/data-lake/prefect/migracao-de-pipeline)
- [Reposit√≥rio GitHub](https://github.com/prefeitura-rio/prefect_rj_iplanrio)
- [Documenta√ß√£o Prefect](https://docs.prefect.io/)

---
